from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark import SparkConf
from pyspark.sql.types import *
from pyspark.sql import SparkSession

AWS_ACCESS_KEY="*****"
AWS_SECRET_KEY="*****"
conf = (SparkConf().set("spark.executor.extraJavaOptions",
                        "-Dcom.amazonaws.services.s3.enableV4=true")
                    .set("spark.driver.extraJavaOptions",
                        "-Dcom.amazonaws.services.s3.enableV4=true")
                    .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)
                    .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)
        )

scT=SparkContext(conf=conf)
sql = SparkSession(scT)



emptyRDD = scT.emptyRDD()
data = [("aaa","bbbb","cccc"),("aaa","bbbb","cccc")]
#data = scT.emptyRDD()
columns = StructType([StructField('Name', StringType(), True),
                    StructField('Age',StringType(), True),
                    StructField('Gender',StringType(), True)])

df = sql.createDataFrame(data,columns)
df.show()

df.write.format('json').mode("overwrite").save("s3a://bucket/path")
