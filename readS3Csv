from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark import SparkConf

conf = (SparkConf().set("spark.executor.extraJavaOptions","-Dcom.amazonaws.services.s3.enableV4=true")
    .set("spark.driver.extraJavaOptions","-Dcom.amazonaws.services.s3.enableV4=true"))

scT=SparkContext(conf=conf)
scT.setSystemProperty("com.amazonaws.services.s3.enableV4", "true")
hadoopConf = scT._jsc.hadoopConfiguration()
hadoopConf.set("fs.s3a.awsAccessKeyId", "***")
hadoopConf.set("fs.s3a.awsSecretAccessKey", "****")


sql = SparkSession(scT)
csv_df = sql.read.csv("s3a://bucketname/CSV/file1.csv")
csv_df.show()
