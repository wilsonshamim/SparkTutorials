Installation of Pyspark in pycharm:

1. Install python 2.7
2. In pycharm File --> settings --> project Interpretor 
    click ADD, set python 2.7
3. In the terminal install pyspark using below command :
    pip install pyspark.
4. download latest version of hadoop and set the HADOOP_HOME=<<path of your hadoop >> in the env variable.
5. once installtion is success, you can test it using a simple python code

import pyspark
sc = pyspark.SparkContext(appName="app2")
print(sc)

you should see output like below:
Picked up JAVA_TOOL_OPTIONS: -Djava.vendor='Sun Microsystems Inc.'
20/05/04 22:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where 
applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
*<SparkContext master=local[*] appName=app2>*



